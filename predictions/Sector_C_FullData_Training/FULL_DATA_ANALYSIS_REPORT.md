# Sector C Bankruptcy Prediction - Full Data Training Analysis

**Analysis Date:** December 3, 2025  
**Scenario:** Trained on complete dataset (45 observations), tested on same data  
**Purpose:** Understand ceiling performance and overfitting magnitude

---

## ğŸ¯ Executive Summary

Training and testing on the **same data** shows what models achieve when they can "memorize" the training set without generalization concerns.

**Key Finding:** Massive performance difference between realistic (80/20 split) and theoretical ceiling (full data):
- **XGBoost & Gradient Boosting:** Perfect 100% accuracy on training data
- **Logistic Regression:** 95.56% accuracy on training data (vs 88.89% on test split)
- **Random Forest:** 95.56% accuracy on training data (vs 77.78% on test split)

**Implication:** EXTREME OVERFITTING - models memorize training data rather than learning generalizable patterns.

---

## ğŸ“Š Detailed Results

### Full Data Training (Ceiling Performance)

All models tested on the SAME data they trained on (45 observations, 6 bankruptcies):

| Model | Accuracy | Precision | Recall | F1 | ROC-AUC | TP | FP | TN | FN |
|-------|----------|-----------|--------|-----|---------|-----|-----|-----|-----|
| **Logistic Regression** | 95.56% | 100% | 66.67% | 80% | 0.9915 | 4 | 0 | 39 | 2 |
| **Random Forest** | 95.56% | 75% | 100% | 85.71% | 1.0000 | 6 | 2 | 37 | 0 |
| **XGBoost** âš ï¸ | **100%** | **100%** | **100%** | **100%** | **1.0000** | 6 | 0 | 39 | 0 |
| **Gradient Boosting** âš ï¸ | **100%** | **100%** | **100%** | **100%** | **1.0000** | 6 | 0 | 39 | 0 |

âš ï¸ Perfect scores on training data = SEVERE OVERFITTING

---

## ğŸ“ˆ Comparison: Full Data vs 80/20 Split

### Accuracy Comparison

```
LOGISTIC REGRESSION
Full Data (Ceiling):  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘ 95.56%
80/20 Split (Real):   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘ 88.89%
Overfitting Gap:      â–ˆâ–ˆâ–‘ +6.67%
                      âš ï¸ MODERATE overfitting

RANDOM FOREST
Full Data (Ceiling):  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘ 95.56%
80/20 Split (Real):   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 77.78%
Overfitting Gap:      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘ +17.78%
                      âš ï¸ SEVERE overfitting

XGBOOST
Full Data (Ceiling):  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 100%
80/20 Split (Real):   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 77.78%
Overfitting Gap:      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘ +22.22%
                      âš ï¸ EXTREME overfitting

GRADIENT BOOSTING
Full Data (Ceiling):  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 100%
80/20 Split (Real):   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 77.78%
Overfitting Gap:      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘ +22.22%
                      âš ï¸ EXTREME overfitting
```

### Precision Comparison

```
LOGISTIC REGRESSION
Full Data (Ceiling):  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘ 100%
80/20 Split (Real):   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 50%
Difference:           â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘ -50%
                      âš ï¸ MASSIVE difference

RANDOM FOREST
Full Data (Ceiling):  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 75%
80/20 Split (Real):   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 33%
Difference:           â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘ -42%
                      âš ï¸ SEVERE difference

XGBOOST & GB
Full Data (Ceiling):  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘ 100%
80/20 Split (Real):   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 33%
Difference:           â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘ -67%
                      âš ï¸ CATASTROPHIC difference
```

### Recall Comparison

```
LOGISTIC REGRESSION
Full Data (Ceiling):  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 66.67%
80/20 Split (Real):   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘ 100%
Difference:           â–ˆâ–ˆâ–ˆâ–ˆâ–‘ +33.33%
                      âœ“ Better on unseen data!

RANDOM FOREST
Full Data (Ceiling):  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘ 100%
80/20 Split (Real):   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘ 100%
Difference:           â–‘ 0%
                      âœ“ Consistent

XGBOOST & GB
Full Data (Ceiling):  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘ 100%
80/20 Split (Real):   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘ 100%
Difference:           â–‘ 0%
                      âœ“ Consistent
```

---

## ğŸ” Key Insights

### 1. **Extreme Overfitting in Tree Models**

**XGBoost & Gradient Boosting show perfect 100% accuracy on training data but only 77.78% on unseen test data.**

This is a RED FLAG indicating:
- Models are memorizing the 45 training examples
- Rules learned are too specific to training data
- Will perform MUCH WORSE on new companies

**Why it happens:**
- Only 45 samples to memorize (very small dataset)
- Tree-based models can overfit easily on small data
- No regularization preventing memorization

### 2. **Logistic Regression More Generalized**

**LR shows 95.56% on training, 88.89% on test (only 6.67% difference).**

This suggests:
- More stable than tree models
- Less prone to memorization
- Better for generalization

**Why it's better:**
- Linear model can't memorize as easily
- Simpler decision boundary = more generalizable
- Regularization helps prevent overfitting

### 3. **Precision vs Recall Tradeoff**

**Full data (perfect recall) vs 80/20 split (perfect precision) shows interesting pattern:**

- Full data: Models catch everything (100% recall) but with false alarms
- 80/20 split: LR prioritizes precision (no false positives)
- Trade-off: Can't have both without more data

### 4. **Feature Importance Instability**

When trained on full data vs 80/20 split, models rank features differently:

**This indicates:** Features don't have stable importance on tiny samples

```
Random Forest Full Data:        Tall 72 > Tall 194 > Tall 1340
Random Forest 80/20 Split:      Tall 72 > Tall 1340 > Tall 85
                                âš ï¸ Different rankings = unstable importance
```

### 5. **What Models Are Actually Memorizing**

With only 45 samples, models can literally memorize which specific companies go bankrupt:
- "If revenue = 5.2M, this is company X that went bankrupt"
- "If this exact pattern of Tall 72 and Tall 85, classify as bankrupt"
- Real bankruptcy patterns get mixed with dataset artifacts

---

## ğŸ“Š Confusion Matrix Analysis

### Full Data Training (Ceiling)

**Logistic Regression:**
```
                 PREDICTED
              Not Bankrupt | Bankrupt
ACTUAL  Not        39      |    0      = 39
        Bankrupt    2      |    4      = 6
                   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    41      |    4
```
- Misses 2 bankruptcies but NO false alarms
- 100% precision (0 false positives)

**XGBoost & Gradient Boosting (Perfect):**
```
                 PREDICTED
              Not Bankrupt | Bankrupt
ACTUAL  Not        39      |    0      = 39
        Bankrupt    0      |    6      = 6
                   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    39      |    6
```
- Catches all 6 bankruptcies
- NO false alarms
- Perfect memorization âš ï¸

### 80/20 Split Testing (Reality)

**Logistic Regression:**
```
                 PREDICTED
              Not Bankrupt | Bankrupt
ACTUAL  Not        7       |    1      = 8
        Bankrupt    0       |    1      = 1
                   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    7       |    2
```
- Catches the 1 bankruptcy (100% recall)
- But 1 false alarm (50% precision)
- More false positives in real-world scenario

---

## âš ï¸ Critical Implications

### For Model Selection

| Model | Full Data | 80/20 Split | Verdict |
|-------|-----------|-----------|---------|
| LR | 95.56% | 88.89% | Gap: 6.67% â†’ STABLE |
| RF | 95.56% | 77.78% | Gap: 17.78% â†’ UNSTABLE |
| XGB | 100% | 77.78% | Gap: 22.22% â†’ SEVERE OVERFITTING |
| GB | 100% | 77.78% | Gap: 22.22% â†’ SEVERE OVERFITTING |

**Best model for generalization: Logistic Regression** (smallest gap between ceiling and reality)

### For Data Requirements

The massive accuracy drop (22.22% for XGBoost) when moving to unseen data shows:
- **Current dataset is TOO SMALL** for tree-based models
- **Tree models need much more data** (500+ observations minimum)
- **Even LR (best model) shows 6.67% degradation**, indicating data scarcity

### For Production Deployment

```
DO NOT DEPLOY THESE MODELS

Reason: Real-world performance (77-89%) is based on models trained
        with 80% of data held out. Actual deployment would see WORSE
        performance because:
        
1. Future data is completely unseen (no overlap with training)
2. Models have shown they overfit on small samples
3. Test set of 9 samples is too tiny to estimate true performance
4. 22.22% accuracy drop (XGBoost) unacceptable for production
```

---

## ğŸ“ˆ Feature Importance Stability

### Comparison Across Scenarios

**Top 5 Features (Full Data Training):**

1. **XGBoost:** Tall 1340 (25.5%), Tall 85 (22.9%), Tall 72 (16.9%), Tall 217 (14.1%), Tall 146 (11.9%)
2. **Gradient Boosting:** Tall 217 (36.96%), Tall 72 (33.42%), Tall 1340 (25.3%)
3. **Random Forest:** Tall 72 (19.0%), Tall 194 (17.0%), Tall 1340 (15.0%), Tall 85 (13.9%)
4. **Logistic Regression:** total_gjeldsgrad (0.617), egenkapitalandel (0.617), Tall 217 (0.526)

**Consensus:** Revenue/income/assets dominate, but exact rankings vary significantly

**Why this matters:** Feature importance rankings on tiny datasets are unreliable. Don't make business decisions based on these rankings without more data.

---

## ğŸ“ Lessons Learned

### What This Analysis Shows

âœ“ **Maximum ceiling performance:** 95-100% accuracy when model can memorize  
âœ“ **Realistic performance:** 77-89% accuracy on unseen data  
âœ“ **Overfitting magnitude:** 17-22% accuracy drop for tree models  
âœ“ **Relative stability:** LR best (only 6.67% drop) vs XGBoost worst (22.22% drop)

### What We Cannot Conclude

âŒ **Real production accuracy:** Haven't tested on truly new, future data  
âŒ **True generalization:** 80/20 split still uses same distribution as training  
âŒ **Stable feature importance:** Rankings too volatile on small samples  
âŒ **Operational reliability:** False positive rate (50-67%) unacceptable

### What We Should Do

1. **Collect 10x more data** before any deployment consideration
2. **Use Logistic Regression** (most stable) until data expands
3. **Monitor false positive rate** in production (currently 50-67%)
4. **Don't trust feature importance** on datasets < 500 observations
5. **Implement human review** layer (can't trust automated predictions yet)

---

## ğŸ“Š Summary Metrics Table

### Performance Degradation (Overfitting Measurement)

| Model | Accuracy Drop | Precision Drop | Recall Change | F1 Drop | ROC-AUC Drop |
|-------|---|---|---|---|---|
| Logistic Regression | -6.67% | -50% | +33% | -12% | -0.0085 |
| Random Forest | -17.78% | -42% | 0% | -36% | -0.25 |
| XGBoost | -22.22% | -67% | 0% | -50% | -0.125 |
| Gradient Boosting | -22.22% | -67% | 0% | -50% | -0.125 |

**Interpretation:**
- LR: Most stable (smallest degradation)
- RF: Moderate degradation
- XGB & GB: Severe degradation (avoid for small datasets)

---

## ğŸ’¡ Recommendations

### Immediate Actions

1. **Do NOT deploy XGBoost or Gradient Boosting** on this dataset
   - 22.22% accuracy degradation is unacceptable
   - Models too prone to memorization with 45 samples

2. **If forced to use any model**, select Logistic Regression
   - Only 6.67% degradation
   - More interpretable
   - Smaller generalization gap

3. **Implement fallback rules**
   - When model confidence < 70%, flag for human review
   - Accounts for limited reliability

### Medium-Term Actions

1. **Expand dataset to 300+ observations** (6-12 months)
2. **Rerun analysis with expanded data**
3. **Then consider tree-based models** (they'll work better with more data)
4. **Implement cross-validation** (5-10 fold instead of single split)

### Long-Term Strategy

1. **Build integrated multi-sector model** using 1000+ observations
2. **Add external features** (commodity prices, economic indicators)
3. **Implement real-time monitoring** (track model drift)
4. **Deploy with human review workflows** (not fully automated)

---

## ğŸ“ Files Generated

- `model_results_full_data.json` - Metrics from full-data training
- `test_predictions.csv` - Predictions on training data
- `feature_importance_*.csv` - Feature rankings (4 files)

---

## Conclusion

This analysis **confirms the fundamental problem:** the 45-observation dataset is too small for reliable machine learning predictions.

The massive accuracy drop when moving from training data to unseen data (6.67% to 22.22%) demonstrates **severe overfitting**, not true predictive power.

**Bottom Line:** Collect more data before considering production deployment. Current models might memorize this specific dataset but will likely fail on new companies.

---

**Analysis Date:** December 3, 2025  
**Data Size:** 45 complete observations, 6 bankruptcy cases  
**Recommendation:** RESEARCH STAGE ONLY - NOT PRODUCTION READY
